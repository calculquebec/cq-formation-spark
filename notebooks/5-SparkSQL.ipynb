{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Data Analysis with Spark SQL\n",
    "\n",
    "Big Data is not only raw text files, a lot of datasets available are structured as table and even raw text files have a underlying structure. Spark SQL allow to query structured datasets, relational database and also provide an API to structure data.\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "1. [Initialization](#1.-Initialization)  \n",
    "  1.1 [Spark](#1.1-Spark)  \n",
    "  1.2 [Spark SQL](#1.2-Spark-SQL)\n",
    "2. [Dataframe](#2.-Dataframe)  \n",
    "  2.1 [Reading Data](#2.1-Reading-Data)  \n",
    "  2.2 [Structuring Data](#2.2-Structuring-Data)  \n",
    "  2.3 [Creating a Dataframe](#2.3-Creating-a-Dataframe)  \n",
    "  2.4 [Registering a Table](#2.4-Registering-a-Table)  \n",
    "  2.5 [Querying Data](#2.5-Querying-Data)  \n",
    "  2.6 [Aggregating Results](#2.6-Aggregating-Results)\n",
    "3. [Writing Results to Disk](#3.-Writing-Results-to-Disk)  \n",
    "  3.1 [Apache Parquet](#3.1-Apache-Parquet)  \n",
    "  3.2 [Other Formats](#3.2-Other-Formats)    \n",
    "4. [Reading the Dataframe from Storage](#4.-Reading-the-Dataframe-from-Storage)  \n",
    "  4.1 [Reading CSV Files](#4.1-Reading-CSV-Files)\n",
    "4. [Ending Spark SQL Analysis](#5.-Ending-Spark-SQL-Analysis)\n",
    "6. [Recap](#6.-Recap)\n",
    "7. [References](#References)\n",
    "\n",
    "## List of Exercises\n",
    "\n",
    "1. [Exercise 1: Initialize Spark](#Exercise-1)\n",
    "2. [Exercise 2: Create an RDD](#Exercise-2)\n",
    "3. [Exercise 3: Count Elements](#Exercise-3)\n",
    "4. [Exercise 4: Transform an RDD](#Exercise-4)\n",
    "5. [Exercise 5: Validate Transformation](#Exercise-5)\n",
    "6. [Exercise 6: Type RDD fields](#Exercise-6)\n",
    "7. [Exercise 7: Return on key-value notions](#Exercise-7)\n",
    "7. [Exercise 8: Create a Dataframe](#Exercise-8)\n",
    "8. [Exercise 9: Display a Dataframe](#Exercise-9)\n",
    "9. [Exercise 10: End the Context](#Exercise-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Spark\n",
    "#### Exercise 1\n",
    "\n",
    "Import the necessary Python module(s) and create a Spark context. \n",
    "\n",
    "**Warning**, verify if there exists a context and handle possible exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.2 Spark SQL\n",
    "\n",
    "We can now import the components that we need to analyze structured data from Spark SQL module `pyspark.sql`.\n",
    "* `SQLContext`: Main entry point for Spark SQL functionality. It will be used to create Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL Context requires a Spark Context as sole argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataframe\n",
    "\n",
    "Textbook definition:    \n",
    "\n",
    "> A data frame is a table, or two-dimensional array-like structure, in which each column contains measurements on one variable, and each row contains one case.\n",
    "\n",
    "In Spark, a dataframe is a distributed collection of data grouped into named columns. It is equivalent to a relational table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Create an RDD with the dataset found in `/scratch/formation/spark/data/pagecounts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Count the number of elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "Transform the previous RDD into a second one where each field originally separated by white spaces are now elements of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Exercise 5\n",
    "\n",
    "To validate the transformation, display the first 8 elements of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "\n",
    "As you can see, the third and fourth fields are numbers represented as text. Transform the RDD in order to convert these strings to `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Structuring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original dataset is strictly text, we now want to give a structure that is define field name and type.\n",
    "\n",
    "\n",
    "A pagecounts file looks like this\n",
    "```\n",
    "af Spesiaal:Onlangse_wysigings 3 101681\n",
    "af Spesiaal:RecentChanges 2 2248\n",
    "af Suid-Afrika 1 30698\n",
    "af Tuisblad 14 155257 \n",
    "af Varkgriep 4 42236\n",
    "af Wikipedia 2 32796\n",
    "```\n",
    "\n",
    "It is a tabular file, where each line is a distinct entry and the columns represent\n",
    "\n",
    "1. the project name (language);\n",
    "2. the page title;\n",
    "3. the number of requests;\n",
    "4. the size of the content returned.\n",
    "\n",
    "To define our structure, we use Spark SQL data types that are defined in [`pyspark.sql.types`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types). We import a small subset of type that we need `LongType` and `StringType`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL also provides two types to defines dataframe structure:\n",
    "- `StructType`: Data type representing a row of a dataframe.\n",
    "- `StructField`: Data type representing a field of a row. It is mainly defined by a name and a type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all these classes, we can define our data schema. The order in the list must correspond to the order in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('lang',    StringType()), \n",
    "                     StructField('name',    StringType()), \n",
    "                     StructField('request', LongType()), \n",
    "                     StructField('size',    LongType())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Creating a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a dataframe, we simply need to invoke the `createDataFrame()` method of our Spark SQL Context and provide it an RDD and our data structure (or schema). \n",
    "\n",
    "#### Exercise 8\n",
    "\n",
    "Replace `<FILL IN>` in the following cell by the proper RDD for `data/pagecounts`, and the second `<FILL IN>` by the method call to persist the dataframe in memory. **Hint**: it is the same method for RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfPageCounts = sqlContext.createDataFrame(<FILL IN>, schema).<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then manipulate the dataframe using its [`pyspark.sql.DataFrame` API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame).\n",
    "\n",
    "We can, for example, show the first lines of the dataframe as a table in ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Registering a Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to query our dataframe with SQL, we need to register it as a table in the Spark SQL context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.registerTempTable(\"page_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Querying Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We are now able to interogate our data using the Structured Query Language (SQL). The following query request the top 10 most requested page in spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.sql(\"SELECT name, request \"\n",
    "                    \"FROM page_table \"\n",
    "                    \"WHERE lang='es' \"\n",
    "                    \"ORDER BY request DESC \"\n",
    "                    \"LIMIT 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A query on a dataframe is a Spark transformation. Therefore, to compute the result, we need to call an action. \n",
    "\n",
    "#### Exercise 9\n",
    "\n",
    "Call the right method to display the dataframe resulting from the preceding query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 SQL 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decompose the preceding query in keywords:\n",
    "\n",
    "#### SELECT\n",
    "\n",
    "Indicate which variable we want to collect. The name of variable have been defined when structuring our data in [section 2.2](#2.2-Structuring-Data).\n",
    "\n",
    "#### FROM\n",
    "\n",
    "Indicate the source of data. The name of the table has been defined in [section 2.4](#2.4-Registering-a-table).\n",
    "\n",
    "#### WHERE\n",
    "\n",
    "Filter the entries based on predicates in function of the variables. \n",
    "\n",
    "#### ORDER BY [...] DESC\n",
    "\n",
    "Indicate we wish to order the resulting dataframe in function of a certain variable, in a certain order. \n",
    "\n",
    "#### LIMIT N \n",
    "\n",
    "Return only a subset of entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 SQL as an API\n",
    "\n",
    "[Dataframe API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) includes methods that are named after SQL keywords. These methods can be used instead of the query language. However, the order does not match exactly as the query is a single statement, the API will return a dataframe after each method call. These calls can be chained to build a similar pipeline.\n",
    "\n",
    "The following example extract and show the 10 most requested pages in English on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.where(\"lang = 'en'\")\\\n",
    "            .select(\"name\", \"request\")\\\n",
    "            .orderBy(\"request\", ascending=False)\\\n",
    "            .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Aggregating Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look back at the preceding results, we realise that some names appear multiple time. The reason is that we have omitted to sum the number of requests per page. We need to aggregate the page entries by name and sum the requests.\n",
    "\n",
    "To do this, we indicate in our query to sum the request (`SUM(request)`) for entries grouped by name (`GROUP BY NAME`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT name, SUM(request)\"\n",
    "               \"FROM page_table \"\n",
    "               \"WHERE lang='en' \"\n",
    "               \"GROUP BY name \"\n",
    "               \"ORDER BY SUM(request) DESC \"\n",
    "               \"LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating Using the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.where(\"lang = 'en'\")\\\n",
    "            .select(\"name\", \"request\")\\\n",
    "            .groupBy('name')\\\n",
    "            .agg({'request' : 'sum'})\\\n",
    "            .orderBy(\"sum(request)\", ascending=False)\\\n",
    "            .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Writing Results to Disk\n",
    "\n",
    "To avoid structuring our data each time, we can save the resulting dataframe to disk. This will preserve the schema and the data order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Apache Parquet\n",
    "\n",
    "[Apache Parquet](https://parquet.apache.org/) is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language. The format is fairly popular with the Spark community as it is efficient and easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.write.parquet(\"pagecounts.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Other Formats\n",
    "\n",
    "Dataframe can also be written as JSON file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfPageCounts.write.json('pagecounts.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also save the dataframe as a table in [Apache Hive](http://hive.apache.org/) or a database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reading the Dataframe from Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new Dataframe can be created by reading back from a file in one of the format we mentionned.\n",
    "\n",
    "For example, to create a dataframe from a Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecount_parq = sqlContext.read.parquet(\"pagecounts.parquet\")\n",
    "pagecount_parq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Reading CSV Files\n",
    "\n",
    "Datasets are often found as CSV files. \n",
    "\n",
    "#### Spark 1.6 [OUTDATED]\n",
    "Unfortunately, Spark does not provide builtin methods to read CSV. However, [Databricks](https://www.databricks.com/), the main company behind Spark developement, provides [spark-csv](https://github.com/databricks/spark-csv), an addon library for parsing and querying CSV data with Apache Spark, for Spark SQL and DataFrames. \n",
    "\n",
    "One wanting to install it himself could consult the following StackOverflow answer to know how : [How to add any new library like spark-csv in Apache Spark prebuilt version](http://stackoverflow.com/questions/30757439/how-to-add-any-new-library-like-spark-csv-in-apache-spark-prebuilt-version).\n",
    "\n",
    "The following line read the employee dataset of a store as a Dataframe and display its content. The dataset is a Tab Separated Values (TSV) file, therefore we need to specify the `delimiter` to Spark as `\\t`, the tab character.\n",
    "\n",
    "``` \n",
    "df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "              .options(header='true', inferschema='true', delimiter='\\t')\\\n",
    "              .load('/scratch/formation/spark/data/store/employees.tsv')\n",
    "df.show()\n",
    "```\n",
    "\n",
    "#### Spark 2.0+\n",
    "\n",
    "The next version of Spark, which you are currently using, has direct support for CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('csv')\\\n",
    "              .options(headers='true', inferSchema='true', delimiter='\\t')\\\n",
    "              .load('/scratch/formation/spark/data/store/employees.tsv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ending Spark SQL Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL's context do not need to be terminated prior to leaving the notebook.\n",
    "\n",
    "#### Exercise 10\n",
    "\n",
    "Terminate the Spark Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recap\n",
    "\n",
    "In this notebook, we put in practice and learned about the following parts of \n",
    "**[Python Spark SQL API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html)**:\n",
    "1. Import Spark SQL Python module: \n",
    "**[`import pyspark.sql`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html)**\n",
    "2. Create a Spark SQLContext:\n",
    "**[`pyspark.sql.SQLContext()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext)**\n",
    "3. Create an RDD from text files:\n",
    "**[`SparkContext.textFile(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile)**\n",
    "4. Count the number of elements in a RDD: \n",
    "**[`Rdd.count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count)**\n",
    "5. Apply a transformation on each element of a RDD:\n",
    "**[`RDD.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map)**\n",
    "6. Take a the first *num* elements from an RDD: \n",
    "**[`Rdd.take(num)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take)**\n",
    "7. Structure and type data fields: **[`pyspark.sql.types`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types)**\n",
    "8. Create a dataframe from an RDD: **[`SQLContext.createDataFrame(RDD)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.createDataFrame)**\n",
    "9. Print the first *n* rows of a dataframe: **[`Dataframe.show(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show)**\n",
    "10. Register a dataframe as a temporary table: **[`DataFrame.registerTempTable(name)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable)**\n",
    "11. Query a context's registered table: **[`SQLContext.sql(name)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.sql)**\n",
    "12. Use the SQL API of a dataframe: **[`DataFrame`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)**\n",
    "13. Write a dataframe as Parquet file: **[`DataFrame.write.parquet(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.parquet)**\n",
    "14. Write a dataframe as JSON file: **[`DataFrame.write.json(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.json)**\n",
    "15. Read a dataframe from a Parquet file: **[`DataFrame.read.parquet(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.parquet)**  \n",
    "16. Read a dataframe from a CSV file: **[`DataFrame.read.format('com.databricks.spark.csv')`](https://github.com/databricks/spark-csv#python-api)**\n",
    "10. End the SparkContext:\n",
    "**[`SparkContext.stop()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.stop)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. References\n",
    "\n",
    "* [Berkeley AmpCamp 5 - Data Exploration Using Spark SQL](http://ampcamp.berkeley.edu/5/exercises/data-exploration-using-spark-sql.html)\n",
    "* [edX - Introduction to Big Data with Apache Spark](https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x)\n",
    "* [edX - Introduction to Big Data with Apache Spark (Github repo)](https://github.com/spark-mooc/mooc-setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
